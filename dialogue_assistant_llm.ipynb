{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz4bD270OQrD"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate==0.21.0 \\\n",
        "  bitsandbytes==0.40.2 \\\n",
        "  peft==0.5.0 \\\n",
        "  transformers==4.34.0 \\\n",
        "  sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFhapQNoOMhV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
        "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
        "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(\n",
        "        self,\n",
        "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
        "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
        "        response_template=DEFAULT_RESPONSE_TEMPLATE\n",
        "    ):\n",
        "        self.message_template = message_template\n",
        "        self.response_template = response_template\n",
        "        self.messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        }]\n",
        "\n",
        "    def add_user_message(self, message):\n",
        "        self.messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def add_bot_message(self, message):\n",
        "        self.messages.append({\n",
        "            \"role\": \"bot\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def get_prompt(self, tokenizer):\n",
        "        final_text = \"\"\n",
        "        for message in self.messages:\n",
        "            message_text = self.message_template.format(**message)\n",
        "            final_text += message_text\n",
        "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
        "        return final_text.strip()\n",
        "\n",
        "\n",
        "def generate(model, tokenizer, prompt, generation_config):\n",
        "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    data = {k: v.to(model.device) for k, v in data.items()}\n",
        "    output_ids = model.generate(\n",
        "        **data,\n",
        "        generation_config=generation_config\n",
        "    )[0]\n",
        "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
        "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    return output.strip()\n",
        "\n",
        "\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "print(generation_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jflXz_W_Ogg3"
      },
      "outputs": [],
      "source": [
        "inputs = [\"Придумай шутку про Аню\", \"Придумай армейскую шутку\", \"Придумай шутку\"]\n",
        "for inp in inputs:\n",
        "    conversation = Conversation()\n",
        "    conversation.add_user_message(inp)\n",
        "    prompt = conversation.get_prompt(tokenizer)\n",
        "\n",
        "    output = generate(model, tokenizer, prompt, generation_config)\n",
        "    print(prompt)\n",
        "    print(output)\n",
        "    print()\n",
        "    print(\"==============================\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}